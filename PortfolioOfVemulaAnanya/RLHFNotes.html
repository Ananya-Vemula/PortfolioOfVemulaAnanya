<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>RLHF Introduction</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      background-color: #f9f9f9;
    }
    h1 {
      color: #333;
    }
    section {
      margin-bottom: 30px;
    }
    ul {
      list-style-type: disc;
      margin-left: 20px;
    }
    .emoji {
      font-size: 1.2em;
    }
  </style>
</head>
<body>

  <h1>RLHF (Reinforcement Learning with Human Feedback)</h1>

  <section>
    <h2>Key Concept</h2>
    <p>RLHF tunes a model's <strong>responses at the response level</strong>, not at the next-token prediction level like standard language modeling.</p>
    <p>It is used in the <strong>alignment phase</strong>, after pretraining, to make the model behave in a way that is helpful, harmless, and honest.</p>
  </section>

  <section>
    <h2>Core Components of RLHF</h2>
    <ul>
      <li><strong>Pretrained Model</strong>: Contains most of the model’s knowledge and capabilities. Alignment focuses on how that knowledge is used.</li>
      <li><strong>Reward Model</strong>: Trained separately using human preferences (ranked outputs). Depends on the application domain.</li>
      <li><strong>Reinforcement Learning Step</strong>: Fine-tuned using PPO or similar algorithms with help from the reward model.</li>
      <li><strong>Contrastive Loss</strong>: Used with negative feedback to help differentiate good and bad responses.</li>
    </ul>
  </section>

  <section>
    <h2>Alignment Hypothesis</h2>
    <p>According to the <strong>Superficial Alignment Hypothesis</strong></p>
    <ul>
      <li>Alignment is mostly about <strong>style and format</strong>, not deep reasoning.</li>
      <li>A small set of examples might be enough to tune a pretrained model.</li>
      <li>This suggests that <strong>alignment ≠ knowledge</strong>.</li>
    </ul>
  </section>

  <section>
    <h2> Milestones in RLHF & Alignment</h2>
    <ul>
      <li><strong>First ChatGPT</strong>: Trained using RLHF.</li>
      <li><strong>Shift in Post-Training Methods</strong>: The DPO (Direct Preference Optimization) method emerged.</li>
      <li><strong>DPO Era Models</strong>: Zephyr-Beta, Tulu 2, and others demonstrated improved alignment through direct preference learning.</li>
    </ul>
    <p>DPO (Direct preference optimization): Directly learns from human preferences without using a reward model; simpler and stable.</p>
    <p>PPO (Proximal policy optimization): A reinforcement learning algorithm commonly used in RLHF with a reward model.</p>
  </section>

  <section>
    <h2> Alternative Feedback Methods</h2>
    <ul>
      <li><strong>TAMER</strong>: Humans iteratively score actions, teaching the agent a reward model.</li>
      <li><strong>COACH</strong>: Human feedback is used to tune the advantage function.</li>
    </ul>
    <p>These two are human in loop methods for RL, usually used in Robotics/RL games and not usually in LLMs. Here a human gives active feedback according to rank or advantage shaping.</p>
  </section>

  <section>
    <h2> Modern Language Model Architecture</h2>
    <ul>
      <li>Uses <strong>decoder-only Transformers</strong> (e.g., GPT architecture).</li>
      <li>Rely on <strong>self-attention mechanisms</strong> to understand and generate language.</li>
    </ul>
  </section>
  <hr>
  <hr>

  <h1>Reward Models in Alignment</h1>
  <p>Reward models are critical components in aligning language models to human preferences by assigning scalar values that represent the quality of a model's output.</p>

  <h2>Types of Reward Models</h2>
  <ul>
    <li><strong>Outcome Reward Models (ORM):</strong>
      <ul>
        <li>Assess the final output of a language model.</li>
        <li>Predicts the probability that a response leads to a correct or desirable outcome.</li>
        <li>Best suited for tasks with clear, single-answer objectives.</li>
      </ul>
    </li>
    <li><strong>Process Reward Models (PRM):</strong>
      <ul>
        <li>Evaluate the intermediate reasoning steps taken by a model.</li>
        <li>Assigns a reward score to each step in the reasoning process.</li>
        <li>Helpful for tasks requiring complex, multi-step reasoning or transparency.</li>
      </ul>
    </li>
  </ul>

  <img src = "imagesfornotes/RewardModelComparision.jpg" alt= "Reward Model Types" style="max-width: 50%; height: auto;">

  <h2>Optimization Techniques for Alignment</h2>
  <ol>
    <li><strong>Reward Modeling:</strong>
      <ul>
        <li>A model is trained using human-labeled preferences.</li>
        <li>Outputs a scalar reward signal for future generations based on quality.</li>
        <li>Used in RLHF setups for guiding policy improvement.</li>
      </ul>
    </li>
    <li><strong>Instruction Fine-tuning:</strong>
      <ul>
        <li>Supervised fine-tuning of language models using curated question-answer pairs.</li>
        <li>Teaches the model the format and style of human-aligned responses.</li>
        <li>Acts as the first stage before reinforcement-based fine-tuning.</li>
      </ul>
    </li>
    <li><strong>Rejection Sampling:</strong>
      <ul>
        <li>A filtering method used to discard poor outputs.</li>
        <li>Only keeps samples that align with human preferences or meet reward thresholds.</li>
        <li>Simple and effective for low-latency deployment or training efficiency.</li>
      </ul>
    </li>
    <li><strong>Policy Gradients:</strong>
      <ul>
        <li>Updates model parameters using the reward model’s signal.</li>
        <li>Part of standard reinforcement learning (e.g., PPO).</li>
        <li>Trains the model to prefer high-reward outputs by maximizing expected reward.</li>
      </ul>
    </li>
    <li><strong>Direct Alignment Algorithms:</strong>
      <ul>
        <li>Optimize model behavior directly from human preferences, bypassing a reward model.</li>
        <li>Example: Direct Preference Optimization (DPO).</li>
        <li>Reduces complexity and instability from intermediate reward estimation.</li>
      </ul>
    </li>
  </ol>

  <p><strong>Conclusion:</strong> These techniques are fundamental in ensuring that large language models not only generate fluent text but also behave in ways aligned with human values and expectations.</p>
  <div style="text-align: center;">
  <img src="imagesfornotes/RLHFDiag.jpg" alt="RLHF Diagram" style="max-width: 70%; height: auto;">
  </div>
  <p> The diagram above shows the following: 
    First, A base model is trained with human + synthetic instructions ie. the initial IFT (Instruction fine tuning), creating an SFT model ie Supervised fine tuned model
    Second, A reward model or an LLM judge is used (made with human preferences), along with the help of PPO or DPO or other multiple optimization techniques to calculate loss, which is send back to retrain the SFT model.
    Finally, we retrain the SFT model, creating a new aligned model after some n iterations.</p>
  
  <hr>
  <hr>
   <title>PPO (Proximal Policy Optimization) - Reinforcement Learning Notes</title>
</head>
<body>
  <h1> PPO (Proximal Policy Optimization) Algorithm</h1>

  <h2> Core Components</h2>
  <table border="1" cellpadding="8" cellspacing="0">
    <tr>
      <th>Component</th>
      <th>Role</th>
    </tr>
    <tr>
      <td>Policy Network</td>
      <td>Takes a state as input and outputs a probability distribution over actions.</td>
    </tr>
    <tr>
      <td>Value Function Network (Critic)</td>
      <td>Takes a state (or state-action pair) and estimates the Q-value (expected return).</td>
    </tr>
  </table>

  <h2>Training Loop (Simplified)</h2>
  <h3>Pass 1 (One batch of trajectories):</h3>
  <ul>
    <li>Use <strong>Policy Network</strong> to generate actions from current states.</li>
    <li>Use <strong>Value Function Network</strong> to estimate Q-values for these actions.</li>
    <li>Compute <strong>Advantage = Actual Return − Estimated Value</strong>
      <ul>
        <li>Actual Return = Sum of discounted future rewards</li>
      </ul>
    </li>
    <li>Compute losses and backpropagate:
      <ul>
        <li><strong>Value Network →</strong> MSE loss from advantage</li>
        <li><strong>Policy Network →</strong> Clipped surrogate objective loss</li>
      </ul>
    </li>
  </ul>

  <h3>Pass 2 and Onward:</h3>
  <ul>
    <li>For each new batch:
      <ul>
        <li>Recalculate MSE loss for value network</li>
        <li>Recalculate clipped objective loss for policy network</li>
        <li>Update both networks</li>
        <li>Repeat for several epochs</li>
      </ul>
    </li>
  </ul>

  <h2>Policy Network Loss (Clipped Objective)</h2>
  <ul>
    <li>Store:
      <ul>
        <li>Old probabilities from previous policy</li>
        <li>Actions and corresponding states</li>
      </ul>
    </li>
    <li>Compute:
      <ul>
        <li><code>ratio = new_prob / old_prob</code></li>
        <li><code>fin1 = ratio × advantage</code></li>
        <li><code>fin2 = clip(ratio, 1 - ε, 1 + ε) × advantage</code></li>
        <li><code>loss = -min(fin1, fin2)</code> (negative for gradient ascent)</li>
      </ul>
    </li>

    <br>
    <br>
    <br>

    <img src="imagesfornotes/PPOdiag.jpg" alt="PPO conclusion" style="max-width: 70%; height: auto;">
    <br>
    <br>
    <br>

    <li>Use this loss to update the policy network parameters.</li>
    
    <br>
    <img src="imagesfornotes/ClippingFormula.jpg" alt="Clipping formula" style="max-width: 70%; height: auto;">

  </ul>

  <h2>Value Network Loss (Critic Loss)</h2>
  <ul>
    <li>Use Mean Squared Error (MSE) between predicted value and actual return.</li>
    <li>Update value function network with this loss.</li>
  </ul>

  <p><strong>Conclusion:</strong> PPO efficiently balances stability and performance by constraining policy updates and optimizing both the actor and critic networks iteratively.</p>

  <hr>
  <hr>
  
   <title>RLHF & PPO - Clarification Points</title>
  <h1>🚨 Key Points You Got Confused About (Clarified)</h1>
  <ul>
    <li><strong>❌ Confusion:</strong> PPO trains the <em>reward model</em>.</li>
    <li><strong>✅ Correction:</strong> PPO does <u>not</u> train the reward model. It trains the <strong>policy network</strong> using signals from a <u>pretrained</u> reward model.</li>

    <li><strong>❌ Confusion:</strong> PPO targets the <em>value network of the base model</em>.</li>
    <li><strong>✅ Correction:</strong> PPO uses a <strong>value network</strong> (critic) that may be a separate head or module, not the base model's own components.</li>

    <li><strong>❌ Confusion:</strong> Reward model and value network are the same.</li>
    <li><strong>✅ Correction:</strong> The <strong>reward model</strong> is trained with human feedback to score outputs. The <strong>value network</strong> is part of PPO's RL framework and estimates expected returns for actions/states.</li>

    <li><strong>❌ Confusion:</strong> PPO works on token-level predictions like standard LM training.</li>
    <li><strong>✅ Correction:</strong> PPO operates at the <strong>response-level</strong>, optimizing complete outputs using reward feedback — not token-by-token.</li>

    <li><strong>❌ Confusion:</strong> PPO is a one-step or static optimization.</li>
    <li><strong>✅ Correction:</strong> PPO is a <strong>multi-step iterative process</strong>, involving repeated sampling, advantage calculation, and gradient updates to both policy and value networks.</li>
  </ul>

  <hr>
  <hr>
   <title>DPO (Direct Preference Optimization)</title>

  <h2>What is DPO?</h2>
  <p>DPO is a post-training alignment method that eliminates the need for a separate reward model by training a language model to directly increase the probability of preferred outputs.</p>

  <h2>Key Steps</h2>
  <ol>
    <li>Pretrain a base LLM (Large Language Model).</li>
    <li>Generate pairs of outputs from the LLM and collect human feedback (as rankings or preferences).</li>
    <li>Train the model to assign:
      <ul>
        <li>High probability to preferred (positive) completions.</li>
        <li>Low probability to less preferred (negative) completions.</li>
      </ul>
    </li>
    <p>Overall, our goal is the 1) Get rid of the reward model and 2) Avoid the model from updating too much.</p>
  </ol>

  <img src = "imagesfornotes/EQ1DPO.jpg" alt = "MainGoals of DPO" style = "max-width: 70%; height: auto;" />


  <h2>Replacement for Reward Model: Bradley-Terry Model</h2>
  <p>Although DPO aims to avoid using a traditional reward model, it does so by modeling the <strong>probability of preference</strong> using the Bradley-Terry (BT) model.</p>

  <p>The BT model uses the <strong>sigmoid function</strong> to convert the log-likelihood difference between two outputs into a probability value between 0 and 1:</p>

  <pre>
    P(prefer A over B) = sigmoid(πₐ - π_b)
  </pre>


  <p>Here, πₐ and π_b are log-likelihoods (or pseudo-reward scores) of the two completions.</p>


  <h2>Preventing Over-Updating: KL Divergence Regularization</h2>
  <p>To ensure the model doesn’t drift too far from the original pretrained distribution, DPO adds a <strong>KL divergence</strong> penalty. This ensures stability and preserves base model knowledge.</p>

  <pre>
    Loss = -log( sigmoid(πₐ - π_b) ) + {β * KL(π_new || π_old)}
  </pre>

  <p><strong>Explanation:</strong></p>
  <ul>
    <li><code>KL(π_new || π_old)</code>: Penalizes the new model for deviating too much from the original (preference-unaware) model.</li>
    <li><code>β</code>: A hyperparameter controlling how strong the penalty is.</li>
  </ul>

  <p>This allows the model to learn human-aligned preferences <strong>without needing a separate reward model</strong>, yet keeping it close to its pretrained distribution.</p>

  <img src = "imagesfornotes/FinalDPO.jpg" alt = "FinalEquation of DPO" style = "max-width: 70%; height: auto;" />

   <title>GRPO and PPO Notes</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 40px;
      background-color: #f9f9f9;
      color: #333;
    }

    h1, h2 {
      color: #2c3e50;
    }

    code {
      background-color: #eee;
      padding: 2px 5px;
      border-radius: 4px;
    }

    ul {
      margin-left: 20px;
    }
  </style>
</head>
<body>

  <h1>GRPO vs PPO: Key Concepts</h1>

  <h2>1. What is GRPO?</h2>
  <p><strong>GRPO</strong> (Gradient-Regularized Policy Optimization) is designed to:</p>
  <ul>
    <li>Improve computational efficiency through mathematical reasoning.</li>
    <li>Optimize memory usage by eliminating the need for a value model.</li>
    <li>Sample multiple outputs per prompt and average their rewards to guide updates.</li>
  </ul>

  <h3>Steps in GRPO</h3>
  <ol>
    <li>Sample <code>n</code> completions from current policy for each prompt.</li>
    <li>Evaluate each with a reward model → get <code>r₁, r₂, ..., rₙ</code>.</li>
    <li>Compute average reward: 
      <div class="formula">
        R̄ = (1/n) × ∑ rᵢ
      </div>
    </li>
    <li>Use policy gradient method to maximize expected reward while regularizing against a reference model.</li>
  </ol>

  <h2>2. Differences from PPO</h2>
  <ul>
    <li><strong>PPO</strong> uses a value model for reward estimation.</li>
    <li><strong>GRPO</strong> eliminates the value model and uses averaged sampled rewards instead.</li>
  </ul>

  <h2>3. Techniques Used</h2>
  <ul>
    <li><strong>Regularization</strong>: Prevents over-optimization by penalizing large deviations from a reference policy.</li>
    <li><strong>Instruction Fine-tuning</strong>: Trains models to follow instructions better.</li>
    <li><strong>Rejection Sampling</strong>: Selects high-quality responses by discarding poor ones.</li>
    <li><strong>Policy Gradient Algorithms</strong>: Core reinforcement learning method to update the policy.</li>
  </ul>

  <h2>4. Regularization using KL-Divergence</h2>
  <p>KL regularization ensures the updated policy doesn’t diverge too much from the reference. The KL divergence is calculated as:</p>

  <p><strong>KL(P || Q)</strong> = ∑ P(x) log [ P(x) / Q(x) ]</p>

  <p>Where:</p>
  <ul>
    <li><code>P(x)</code> = new policy</li>
    <li><code>Q(x)</code> = reference policy (e.g., initial supervised model)</li>
  </ul>

  <h1>Understanding Instruction Finetuning (IFT)</h1>

  <h2>What is Instruction Finetuning (IFT)?</h2>
  <p>Imagine you have a super smart student who knows a lot of general stuff, but they're not very good at following specific instructions yet. Instruction Finetuning is like giving that student a special training course where they learn to understand and follow instructions perfectly.</p>
  <p>In the world of AI, IFT teaches a large language model (LLM) to be really good at understanding your "instructions" or "prompts" and giving you the kind of response you expect. It uses a specific way of organizing the information we give to the AI, like a script for a play.</p>

  <h2>How Does IFT Work? (Chat Templates and Roles)</h2>
  <p>Think of it like a conversation with different people involved. In IFT, every piece of information we give to the AI is assigned a "role." There are three main roles:</p>

  <ul>
    <li>
      <p><strong class="role role-system">System:</strong> This is like setting the overall mood or rules for the AI. It's the prompt that tells the AI how to behave or what kind of personality it should have. It's often hidden from the user, but it's super important for guiding the AI's behavior.</p>
      <div class="example-box">
        <strong>Example:</strong> <span class="role-system">"You are a helpful and friendly assistant. Always provide concise answers."</span>
      </div>
    </li>
    <li>
      <p><strong class="role role-user">User:</strong> This is YOU! It's any message or question you send to the AI. This is where you give your instructions or ask for information.</p>
      <div class="example-box">
        <strong>Example:</strong> <span class="role-user">"Can you explain photosynthesis to me in simple terms?"</span>
      </div>
    </li>
    <li>
      <p><strong class="role role-assistant">Assistant:</strong> This is the AI's response back to you. It holds what the AI says or generates based on your instructions and the system's guidance.</p>
      <div class="example-box">
        <strong>Example:</strong> <span class="role-assistant">"Sure! Photosynthesis is how plants make their own food using sunlight, water, and carbon dioxide."</span>
      </div>
    </li>
  </ul>

  <p>These roles create a structured "chat template" where the AI learns how different parts of a conversation fit together. It's like teaching a child the difference between who asks a question, who gives an answer, and what the general rules of polite conversation are.</p>

  <h2>Why is IFT Important? (Key Principles)</h2>
  <p>There are a couple of big ideas that make IFT so powerful:</p>

  <ol>
    <li>
      <p><strong>High-Quality Data is King:</strong> Imagine trying to teach someone to cook with bad recipes. They'll never learn! The same goes for AI. The better and more accurate the examples (the "data" in our notes) we give the AI during IFT, the better it will perform. If we show it lots of good instructions and good answers, it learns to give good answers itself.</p>
    </li>
    <li>
      <p><strong>Lots of Practice Makes Perfect (~1 Million Prompts!):</strong> It takes a lot of practice for an AI to become truly amazing. We're talking about feeding it around <strong>1 million (that's 1,000,000!)</strong> different instructions or prompts during the IFT process. This massive amount of training helps the model become super capable, especially for something called <strong>RLHF (Reinforcement Learning from Human Feedback)</strong>, which is another advanced training technique that makes AI even better at being helpful and harmless.</p>
    </li>
  </ol>

  <p>So, in simple terms, Instruction Finetuning is like a specialized training program that teaches AI models to understand and follow instructions by breaking down conversations into specific roles and practicing with a huge amount of high-quality examples. This is a big reason why today's AI can understand your questions and respond so intelligently!</p>
 
  
  <h1>Advanced AI Finetuning Techniques - A Noob's Guide</h1>

  <h2>Rejection Sampling (RS)</h2>
  <p>Imagine you're trying to pick the best apples from a huge orchard. You can't check every single one. Rejection Sampling is a clever way to find the "best" responses from an AI model without checking every single possible answer it could generate. It's a common and simple technique used to make AI models better at giving preferred answers.</p>

  <h3>How it Works:</h3>
  <ol>
    <li>The AI model generates many different answers (let's call them "completions") for the same question or instruction.</li>
    <li>A special "reward model" (another AI model trained to judge quality) evaluates each of these generated answers and gives them a "score" or "reward."</li>
    <li>We then pick only the best-scoring answers to use for further training. This is like only keeping the juiciest, crunchiest apples you found.</li>
    <li>Finally, we take these "top" answers and use them to finetune (further train) the original AI model, teaching it to generate more of these high-quality responses in the future.</li>
  </ol>

  <p>Many research papers use Rejection Sampling as a starting point or a comparison (a "baseline") because it's a straightforward way to improve an AI model's output based on quality.</p>

  <h3>Different Ways to "Pick the Best" in Rejection Sampling:</h3>
  <p>When we say "top completions," there are a few ways to decide what "top" means:</p>

  <h4>1) Selecting Top-N Completions</h4>
  <ul>
    <li>This is like saying, "Give me the <strong>best N</strong> (e.g., top 10, top 100) answers from <strong>all the answers generated for all questions combined</strong>."</li>
    <li>You collect all the possible answers the AI generated for a bunch of different questions, give them all a score with your reward model, and then just take the absolute top N answers overall, regardless of which specific question they came from.</li>
    <li>This method prioritizes overall highest quality across the entire dataset.</li>
  </ul>

  <h4>2) Top Per Prompt</h4>
  <ul>
    <li>This method is more about fairness across different questions. For each question (or "prompt"), you generate several answers.</li>
    <li>Then, for <span class="highlight">each individual question</span>, you pick the best answer that the AI generated specifically for that question.</li>
    <li>So, if you asked 10 questions and generated 5 answers for each, you'd end up with 10 "best" answers (one for each question), even if some of them aren't as high-scoring as the absolute best answer from a different question.</li>
    <div class="example-box">
      <strong>Example:</strong>
      <ul>
        <li><strong>Prompt 1:</strong> "Tell me about dogs."
          <ul>
            <li>Answer A: "Dogs are fluffy." (Reward: 0.7)</li>
            <li>Answer B: "Dogs are domesticated mammals." (Reward: 0.9) <-- Pick this one for Prompt 1</li>
          </ul>
        </li>
        <li><strong>Prompt 2:</strong> "What is photosynthesis?"
          <ul>
            <li>Answer C: "It's how plants make food." (Reward: 0.8) <-- Pick this one for Prompt 2</li>
            <li>Answer D: "Photosynthesis is a complex biochemical process..." (Reward: 0.6)</li>
          </ul>
        </li>
      </ul>
      In "Top Per Prompt," we'd pick Answer B for Prompt 1 and Answer C for Prompt 2.
    </div>
  </ul>

  <h4>3) Top Overall Prompts (less common in simple RS, but related to data selection)</h4>
  <ul>
    <li>This isn't about picking top *completions* as much as it is about identifying the *prompts* that lead to the most valuable responses.</li>
    <li>Sometimes, you might have certain questions or types of questions that are more important or lead to more interesting/complex answers.</li>
    <li>You might use the reward model to evaluate all answers generated for a given prompt, and then decide to prioritize training on prompts that consistently generated high-quality answers, or perhaps prompts where the model struggled but then produced a good answer after several tries.</li>
    <li>This is more about curating the <strong>input side</strong> of the data.</li>
  </ul>

  <h2>Policy Gradient Algorithms</h2>
  <p>Alright, let's switch gears to another core concept in making AI smart: Policy Gradient algorithms. Imagine you're teaching a robot to walk. You don't tell it exactly where to place each foot (that's too much detail!). Instead, you set up a system where it tries different ways of walking, and if it moves forward and doesn't fall, it gets a "reward." Policy Gradient algorithms work similarly: they teach an AI agent to learn the best "strategy" or "policy" to maximize its rewards over time.</p>

  <h3>What is a "Policy" in AI?</h3>
  <p>In Reinforcement Learning (RL), the "policy" is the AI agent's strategy. It's like a brain that tells the agent what action to take in any given situation (or "state"). For example, in a game, the policy might tell the AI: "If the enemy is here, move left."</p>
  <p>Policy Gradient methods directly optimize this policy. Instead of trying to figure out the "value" of every single action in every single situation (which can be impossible in complex scenarios), they adjust the policy itself to make it more likely to choose actions that lead to higher rewards.</p>

  <h3>The Core Idea: Learning by "Trial and Error" with a Compass</h3>
  <ol>
    <li><strong>Trial:</strong> The AI agent tries out actions based on its current policy in an environment (e.g., the robot tries a step).</li>
    <li><strong>Error (or Success!):</strong> It receives a reward based on whether those actions were good or bad (e.g., positive reward for moving forward, negative for falling).</li>
    <li><strong>Compass (Gradient):</strong> Policy Gradient algorithms calculate a "gradient." Think of this as a compass that points in the direction of "more rewards."</li>
    <li><strong>Adjustment:</strong> The AI adjusts its policy slightly in the direction the compass points. This makes it more likely to repeat actions that led to good rewards and less likely to repeat actions that led to bad ones. This adjustment is called <span class="highlight">gradient ascent</span>, because we're trying to increase (ascend) the total reward.</li>
  </ol>

  <h3>Why are Policy Gradients Important?</h3>
  <ul>
    <li><strong>Direct Policy Optimization:</strong> Unlike other methods that try to estimate the value of actions first, Policy Gradients directly improve the action-choosing strategy. This is useful for complex problems where figuring out exact "values" is too hard.</li>
    <li><strong>Handles Continuous Actions:</strong> Imagine controlling a robot arm where you can set the joint angle to any value (not just "bend" or "straighten"). Policy Gradients can handle these "continuous" actions much better than methods that need to list every single possible action.</li>
    <li><strong>Stochastic Policies:</strong> Policy Gradient methods can learn "stochastic" policies, meaning they output probabilities for actions. This allows the AI to explore different actions even if they aren't immediately the "best" choice, which helps it find better long-term solutions and avoid getting stuck in a "local optimum" (a good but not perfect solution).</li>
    <li><strong>Foundation for RLHF:</strong> Policy Gradients are a fundamental building block for advanced techniques like Reinforcement Learning from Human Feedback (RLHF), which is used to align powerful language models (like the one you're interacting with) with human preferences and safety guidelines.</li>
  </ul>

  <h3>Key Algorithms (Examples):</h3>
  <ul>
    <li><strong>REINFORCE:</strong> A foundational policy gradient algorithm that uses complete sequences of actions and rewards (called "trajectories" or "episodes") to calculate the gradient. It's like reviewing an entire game to see what moves led to winning.</li>
    <li><strong>Actor-Critic Methods:</strong> These are more advanced and combine two parts: an "Actor" (the policy that decides what actions to take) and a "Critic" (which evaluates how good those actions were, helping to guide the Actor's learning more efficiently).</li>
    <li><strong>Proximal Policy Optimization (PPO):</strong> A very popular and robust policy gradient algorithm known for its stability and good performance. It's often used in large-scale AI training.</li>
  </ul>

  <p>In short, Policy Gradient algorithms are a powerful family of methods that directly teach AI agents to learn optimal strategies by trying things out, getting feedback (rewards), and adjusting their behavior towards higher rewards, making them excellent for a wide range of complex tasks from robotics to playing games and generating human-like text.</p>

    <h1>Direct Alignment Algorithms and RLVR in RLHF</h1>

  <p>When we talk about making AI models, especially large language models (LLMs), helpful and aligned with what humans want, we often refer to **Reinforcement Learning from Human Feedback (RLHF)**. Traditionally, RLHF involves a few key steps:</p>
  <ol>
    <li>You start with a powerful language model that can generate text.</li>
    <li>You collect data where humans rank or compare different outputs from the model.</li>
    <li>You train a separate AI model, called a **"reward model,"** to learn these human preferences. This reward model then gives scores to new responses.</li>
    <li>Finally, you use a Reinforcement Learning (RL) algorithm (like PPO, which we discussed earlier) to finetune the original language model, teaching it to generate text that gets high scores from the reward model.</li>
  </ol>
  <p>Now, let's explore some newer ways to achieve this alignment that simplify or change this traditional pipeline.</p>

  ---

  <h2>Direct Alignment Algorithms</h2>
  <p><strong>Direct Alignment Algorithms</strong> are a more streamlined approach to RLHF. Their goal is to update an AI model to achieve the same alignment objective as traditional RLHF, but **without ever having to train an intermediate reward model or use complex RL optimizers.**</p>

  <h3>Why are they "Direct"?</h3>
  <ul>
    <li>They **don't train a separate reward model**: Instead of training a dedicated AI to score responses, these algorithms directly use human preference data (e.g., "this response is better than that one") to update the main language model.</li>
    <li>They **don't rely on complex RL optimizers**: Traditional RL algorithms like PPO can be quite challenging to implement and tune. Direct alignment methods often convert the preference learning problem into a simpler optimization task that can be solved with more standard neural network training techniques (like those used in supervised learning).</li>
  </ul>

  <h3>Example: DPO (Direct Preference Optimization)</h3>
  <p><strong>DPO</strong> is a leading and very popular example of a direct alignment algorithm. Here's a simplified idea of how it works:</p>
  <ol>
    <li>You need a dataset of **"preferred" and "rejected" response pairs**. This data is typically created by showing humans different outputs from an AI model for the same prompt and asking them to choose which one they prefer.</li>
    <li>DPO then trains the language model to **increase the probability of generating the "preferred" response** and **decrease the probability of generating the "rejected" response** for any given prompt.</li>
    <li>The clever part is that DPO rephrases the entire RLHF objective (maximizing a reward based on preferences) into a simple, single-stage loss function that can be optimized using basic gradient descent, much like how you'd train a model for classification. This avoids the separate reward model and the complexities of RL.</li>
  </ol>
  <div class="example-box">
    <strong>DPO in Action (Simplified):</strong>
    <ul>
      <li><strong>Prompt:</strong> "Write a short poem about stars."</li>
      <li><strong>Preferred Response ($Y_W$ - "Winner"):</strong> "Twinkling diamonds, shy and bright, / Jewels scattered through the night."</li>
      <li><strong>Rejected Response ($Y_L$ - "Loser"):</strong> "Stars are really hot. They are far away."</li>
    </ul>
    DPO adjusts the model's parameters so that, given the prompt, it's more likely to produce the poem and less likely to produce the factual but unpoetic sentence.
  </div>

  <h3>Advantages of Direct Alignment Algorithms:</h3>
  <ul>
    <li><strong>Simplicity:</strong> They're generally easier to implement and train than the multi-stage RLHF pipeline.</li>
    <li><strong>Stability:</strong> Often more stable during training because they avoid some of the complexities and hyperparameters of RL.</li>
    <li><strong>Efficiency:</strong> Can be more computationally efficient since you don't need to train and maintain a separate reward model.</li>
    <li><strong>Strong Performance:</strong> Despite their simplicity, DPO and similar methods often achieve comparable or even better performance than traditional RLHF on various alignment tasks.</li>
  </ul>

  ---

  <h2>RLVR: Reinforcement Learning with Verifiable Rewards</h2>
  <p>While Direct Alignment Algorithms aim for simplicity in the RLHF pipeline, **RLVR (Reinforcement Learning with Verifiable Rewards)** takes a different approach to the **"reward"** part of RLHF. Traditional RLHF relies on **subjective human preferences** to train a reward model. This can be costly, slow, and sometimes inconsistent due to human bias or disagreement.</p>

  <p>RLVR offers an alternative: instead of relying on human preferences and training a subjective reward model, it uses **objective, verifiable feedback** for its rewards. This means the feedback is either definitively "correct" or "wrong," based on external tools or predefined rules, not human opinion.</p>

  <h3>How it Works:</h3>
  <p>In RLVR, the reward signal for the AI model comes from automatic, external sources:</p>
  <ul>
    <li><strong>Verifiers:</strong> These are specialized tools or programs that can check the correctness of an AI's output automatically.
      <ul>
        <li><strong>Example 1 (Math):</strong> If an LLM is asked to solve a math problem (e.g., "$2 + 2 = ?$"), a **mathematical solver** can verify if the generated answer ("4") is indeed correct. The AI gets a positive reward for "4" and a zero/negative reward for "5".</li>
        <li><strong>Example 2 (Code):</strong> If an LLM generates code, a **compiler or test suite** can verify if the code compiles and runs successfully, or if it passes specific test cases.</li>
      </ul>
    </li>
    <li><strong>Rule-Based Tools:</strong> Pre-defined, unambiguous rules that determine if an output meets certain criteria.
      <ul>
        <li><strong>Example:</strong> "Does the generated response contain specific keywords?"</li>
        <li><strong>Example:</strong> "Is the generated text within a certain character limit?"</li>
      </ul>
    </li>
  </ul>
  <p>This "correct" or "wrong" feedback (often a simple binary 1 or 0) is then used as the reward signal for a standard reinforcement learning algorithm to train the language model.</p>

  <h3>Key Differences and Advantages of RLVR:</h3>
  <ul>
    <li><strong>Objective Rewards:</strong> The rewards are based on facts or rules, making them highly consistent and reliable, unlike subjective human judgments.</li>
    <li><strong>Scalability:</strong> Automating the reward generation process makes training much faster and cheaper, reducing the need for extensive human annotation.</li>
    <li><strong>Specificity:</strong> RLVR excels in tasks where correctness can be precisely defined and automatically checked, such as scientific reasoning, coding, or logical problem-solving.</li>
    <li><strong>Reduced Human Bias:</strong> By removing direct human judgment from the reward signal, RLVR can potentially reduce certain biases that might be present in human-preferred datasets.</li>
  </ul>

  <h3>When is RLVR Most Useful?</h3>
  <p>RLVR is particularly powerful for tasks where there's a clear, objective right or wrong answer. For example, if you want your LLM to be excellent at coding, solving equations, or generating factually accurate data that can be programmatically verified, RLVR is a strong candidate. However, for tasks that require creativity, nuance, or subjective style (like writing a compelling story or having a friendly conversation), human feedback is still essential, and traditional RLHF or direct alignment algorithms might be more suitable.</p>

  <p>Both direct alignment algorithms and RLVR are pushing the boundaries of how we train and align AI models, each offering unique benefits for different alignment challenges.</p>


</body>
</html>
