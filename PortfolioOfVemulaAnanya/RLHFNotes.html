<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>RLHF Introduction</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      background-color: #f9f9f9;
    }
    h1 {
      color: #333;
    }
    section {
      margin-bottom: 30px;
    }
    ul {
      list-style-type: disc;
      margin-left: 20px;
    }
    .emoji {
      font-size: 1.2em;
    }
  </style>
</head>
<body>

  <h1>RLHF (Reinforcement Learning with Human Feedback)</h1>

  <section>
    <h2>Key Concept</h2>
    <p>RLHF tunes a model's <strong>responses at the response level</strong>, not at the next-token prediction level like standard language modeling.</p>
    <p>It is used in the <strong>alignment phase</strong>, after pretraining, to make the model behave in a way that is helpful, harmless, and honest.</p>
  </section>

  <section>
    <h2>Core Components of RLHF</h2>
    <ul>
      <li><strong>Pretrained Model</strong>: Contains most of the model‚Äôs knowledge and capabilities. Alignment focuses on how that knowledge is used.</li>
      <li><strong>Reward Model</strong>: Trained separately using human preferences (ranked outputs). Depends on the application domain.</li>
      <li><strong>Reinforcement Learning Step</strong>: Fine-tuned using PPO or similar algorithms with help from the reward model.</li>
      <li><strong>Contrastive Loss</strong>: Used with negative feedback to help differentiate good and bad responses.</li>
    </ul>
  </section>

  <section>
    <h2>Alignment Hypothesis</h2>
    <p>According to the <strong>Superficial Alignment Hypothesis</strong></p>
    <ul>
      <li>Alignment is mostly about <strong>style and format</strong>, not deep reasoning.</li>
      <li>A small set of examples might be enough to tune a pretrained model.</li>
      <li>This suggests that <strong>alignment ‚â† knowledge</strong>.</li>
    </ul>
  </section>

  <section>
    <h2> Milestones in RLHF & Alignment</h2>
    <ul>
      <li><strong>First ChatGPT</strong>: Trained using RLHF.</li>
      <li><strong>Shift in Post-Training Methods</strong>: The DPO (Direct Preference Optimization) method emerged.</li>
      <li><strong>DPO Era Models</strong>: Zephyr-Beta, Tulu 2, and others demonstrated improved alignment through direct preference learning.</li>
    </ul>
    <p>DPO (Direct preference optimization): Directly learns from human preferences without using a reward model; simpler and stable.</p>
    <p>PPO (Proximal policy optimization): A reinforcement learning algorithm commonly used in RLHF with a reward model.</p>
  </section>

  <section>
    <h2> Alternative Feedback Methods</h2>
    <ul>
      <li><strong>TAMER</strong>: Humans iteratively score actions, teaching the agent a reward model.</li>
      <li><strong>COACH</strong>: Human feedback is used to tune the advantage function.</li>
    </ul>
    <p>These two are human in loop methods for RL, usually used in Robotics/RL games and not usually in LLMs. Here a human gives active feedback according to rank or advantage shaping.</p>
  </section>

  <section>
    <h2> Modern Language Model Architecture</h2>
    <ul>
      <li>Uses <strong>decoder-only Transformers</strong> (e.g., GPT architecture).</li>
      <li>Rely on <strong>self-attention mechanisms</strong> to understand and generate language.</li>
    </ul>
  </section>
  <hr>
  <hr>

  <h1>Reward Models in Alignment</h1>
  <p>Reward models are critical components in aligning language models to human preferences by assigning scalar values that represent the quality of a model's output.</p>

  <h2>Types of Reward Models</h2>
  <ul>
    <li><strong>Outcome Reward Models (ORM):</strong>
      <ul>
        <li>Assess the final output of a language model.</li>
        <li>Predicts the probability that a response leads to a correct or desirable outcome.</li>
        <li>Best suited for tasks with clear, single-answer objectives.</li>
      </ul>
    </li>
    <li><strong>Process Reward Models (PRM):</strong>
      <ul>
        <li>Evaluate the intermediate reasoning steps taken by a model.</li>
        <li>Assigns a reward score to each step in the reasoning process.</li>
        <li>Helpful for tasks requiring complex, multi-step reasoning or transparency.</li>
      </ul>
    </li>
  </ul>

  <img src = "imagesfornotes/RewardModelComparision.jpg" alt= "Reward Model Types" style="max-width: 50%; height: auto;">

  <h2>Optimization Techniques for Alignment</h2>
  <ol>
    <li><strong>Reward Modeling:</strong>
      <ul>
        <li>A model is trained using human-labeled preferences.</li>
        <li>Outputs a scalar reward signal for future generations based on quality.</li>
        <li>Used in RLHF setups for guiding policy improvement.</li>
      </ul>
    </li>
    <li><strong>Instruction Fine-tuning:</strong>
      <ul>
        <li>Supervised fine-tuning of language models using curated question-answer pairs.</li>
        <li>Teaches the model the format and style of human-aligned responses.</li>
        <li>Acts as the first stage before reinforcement-based fine-tuning.</li>
      </ul>
    </li>
    <li><strong>Rejection Sampling:</strong>
      <ul>
        <li>A filtering method used to discard poor outputs.</li>
        <li>Only keeps samples that align with human preferences or meet reward thresholds.</li>
        <li>Simple and effective for low-latency deployment or training efficiency.</li>
      </ul>
    </li>
    <li><strong>Policy Gradients:</strong>
      <ul>
        <li>Updates model parameters using the reward model‚Äôs signal.</li>
        <li>Part of standard reinforcement learning (e.g., PPO).</li>
        <li>Trains the model to prefer high-reward outputs by maximizing expected reward.</li>
      </ul>
    </li>
    <li><strong>Direct Alignment Algorithms:</strong>
      <ul>
        <li>Optimize model behavior directly from human preferences, bypassing a reward model.</li>
        <li>Example: Direct Preference Optimization (DPO).</li>
        <li>Reduces complexity and instability from intermediate reward estimation.</li>
      </ul>
    </li>
  </ol>

  <p><strong>Conclusion:</strong> These techniques are fundamental in ensuring that large language models not only generate fluent text but also behave in ways aligned with human values and expectations.</p>
  <div style="text-align: center;">
  <img src="imagesfornotes/RLHFDiag.jpg" alt="RLHF Diagram" style="max-width: 70%; height: auto;">
  </div>
  <p> The diagram above shows the following: 
    First, A base model is trained with human + synthetic instructions ie. the initial IFT (Instruction fine tuning), creating an SFT model ie Supervised fine tuned model
    Second, A reward model or an LLM judge is used (made with human preferences), along with the help of PPO or DPO or other multiple optimization techniques to calculate loss, which is send back to retrain the SFT model.
    Finally, we retrain the SFT model, creating a new aligned model after some n iterations.</p>
  
  <hr>
  <hr>
   <title>PPO (Proximal Policy Optimization) - Reinforcement Learning Notes</title>
</head>
<body>
  <h1> PPO (Proximal Policy Optimization) Algorithm</h1>

  <h2> Core Components</h2>
  <table border="1" cellpadding="8" cellspacing="0">
    <tr>
      <th>Component</th>
      <th>Role</th>
    </tr>
    <tr>
      <td>Policy Network</td>
      <td>Takes a state as input and outputs a probability distribution over actions.</td>
    </tr>
    <tr>
      <td>Value Function Network (Critic)</td>
      <td>Takes a state (or state-action pair) and estimates the Q-value (expected return).</td>
    </tr>
  </table>

  <h2>Training Loop (Simplified)</h2>
  <h3>Pass 1 (One batch of trajectories):</h3>
  <ul>
    <li>Use <strong>Policy Network</strong> to generate actions from current states.</li>
    <li>Use <strong>Value Function Network</strong> to estimate Q-values for these actions.</li>
    <li>Compute <strong>Advantage = Actual Return ‚àí Estimated Value</strong>
      <ul>
        <li>Actual Return = Sum of discounted future rewards</li>
      </ul>
    </li>
    <li>Compute losses and backpropagate:
      <ul>
        <li><strong>Value Network ‚Üí</strong> MSE loss from advantage</li>
        <li><strong>Policy Network ‚Üí</strong> Clipped surrogate objective loss</li>
      </ul>
    </li>
  </ul>

  <h3>Pass 2 and Onward:</h3>
  <ul>
    <li>For each new batch:
      <ul>
        <li>Recalculate MSE loss for value network</li>
        <li>Recalculate clipped objective loss for policy network</li>
        <li>Update both networks</li>
        <li>Repeat for several epochs</li>
      </ul>
    </li>
  </ul>

  <h2>Policy Network Loss (Clipped Objective)</h2>
  <ul>
    <li>Store:
      <ul>
        <li>Old probabilities from previous policy</li>
        <li>Actions and corresponding states</li>
      </ul>
    </li>
    <li>Compute:
      <ul>
        <li><code>ratio = new_prob / old_prob</code></li>
        <li><code>fin1 = ratio √ó advantage</code></li>
        <li><code>fin2 = clip(ratio, 1 - Œµ, 1 + Œµ) √ó advantage</code></li>
        <li><code>loss = -min(fin1, fin2)</code> (negative for gradient ascent)</li>
      </ul>
    </li>

    <br>
    <br>
    <br>

    <img src="imagesfornotes/PPOdiag.jpg" alt="PPO conclusion" style="max-width: 70%; height: auto;">
    <br>
    <br>
    <br>

    <li>Use this loss to update the policy network parameters.</li>
    
    <br>
    <img src="imagesfornotes/ClippingFormula.jpg" alt="Clipping formula" style="max-width: 70%; height: auto;">

  </ul>

  <h2>Value Network Loss (Critic Loss)</h2>
  <ul>
    <li>Use Mean Squared Error (MSE) between predicted value and actual return.</li>
    <li>Update value function network with this loss.</li>
  </ul>

  <p><strong>Conclusion:</strong> PPO efficiently balances stability and performance by constraining policy updates and optimizing both the actor and critic networks iteratively.</p>

  <hr>
  <hr>
  
   <title>RLHF & PPO - Clarification Points</title>
  <h1>üö® Key Points You Got Confused About (Clarified)</h1>
  <ul>
    <li><strong>‚ùå Confusion:</strong> PPO trains the <em>reward model</em>.</li>
    <li><strong>‚úÖ Correction:</strong> PPO does <u>not</u> train the reward model. It trains the <strong>policy network</strong> using signals from a <u>pretrained</u> reward model.</li>

    <li><strong>‚ùå Confusion:</strong> PPO targets the <em>value network of the base model</em>.</li>
    <li><strong>‚úÖ Correction:</strong> PPO uses a <strong>value network</strong> (critic) that may be a separate head or module, not the base model's own components.</li>

    <li><strong>‚ùå Confusion:</strong> Reward model and value network are the same.</li>
    <li><strong>‚úÖ Correction:</strong> The <strong>reward model</strong> is trained with human feedback to score outputs. The <strong>value network</strong> is part of PPO's RL framework and estimates expected returns for actions/states.</li>

    <li><strong>‚ùå Confusion:</strong> PPO works on token-level predictions like standard LM training.</li>
    <li><strong>‚úÖ Correction:</strong> PPO operates at the <strong>response-level</strong>, optimizing complete outputs using reward feedback ‚Äî not token-by-token.</li>

    <li><strong>‚ùå Confusion:</strong> PPO is a one-step or static optimization.</li>
    <li><strong>‚úÖ Correction:</strong> PPO is a <strong>multi-step iterative process</strong>, involving repeated sampling, advantage calculation, and gradient updates to both policy and value networks.</li>
  </ul>

  <hr>
  <hr>
   <title>DPO (Direct Preference Optimization)</title>

  <h2>What is DPO?</h2>
  <p>DPO is a post-training alignment method that eliminates the need for a separate reward model by training a language model to directly increase the probability of preferred outputs.</p>

  <h2>Key Steps</h2>
  <ol>
    <li>Pretrain a base LLM (Large Language Model).</li>
    <li>Generate pairs of outputs from the LLM and collect human feedback (as rankings or preferences).</li>
    <li>Train the model to assign:
      <ul>
        <li>High probability to preferred (positive) completions.</li>
        <li>Low probability to less preferred (negative) completions.</li>
      </ul>
    </li>
    <p>Overall, our goal is the 1) Get rid of the reward model and 2) Avoid the model from updating too much.</p>
  </ol>

  <img src = "imagesfornotes/EQ1DPO.jpg" alt = "MainGoals of DPO" style = "max-width: 70%; height: auto;" />


  <h2>Replacement for Reward Model: Bradley-Terry Model</h2>
  <p>Although DPO aims to avoid using a traditional reward model, it does so by modeling the <strong>probability of preference</strong> using the Bradley-Terry (BT) model.</p>

  <p>The BT model uses the <strong>sigmoid function</strong> to convert the log-likelihood difference between two outputs into a probability value between 0 and 1:</p>

  <pre>
    P(prefer A over B) = sigmoid(œÄ‚Çê - œÄ_b)
  </pre>


  <p>Here, œÄ‚Çê and œÄ_b are log-likelihoods (or pseudo-reward scores) of the two completions.</p>


  <h2>Preventing Over-Updating: KL Divergence Regularization</h2>
  <p>To ensure the model doesn‚Äôt drift too far from the original pretrained distribution, DPO adds a <strong>KL divergence</strong> penalty. This ensures stability and preserves base model knowledge.</p>

  <pre>
    Loss = -log( sigmoid(œÄ‚Çê - œÄ_b) ) + {Œ≤ * KL(œÄ_new || œÄ_old)}
  </pre>

  <p><strong>Explanation:</strong></p>
  <ul>
    <li><code>KL(œÄ_new || œÄ_old)</code>: Penalizes the new model for deviating too much from the original (preference-unaware) model.</li>
    <li><code>Œ≤</code>: A hyperparameter controlling how strong the penalty is.</li>
  </ul>

  <p>This allows the model to learn human-aligned preferences <strong>without needing a separate reward model</strong>, yet keeping it close to its pretrained distribution.</p>

  <img src = "imagesfornotes/FinalDPO.jpg" alt = "FinalEquation of DPO" style = "max-width: 70%; height: auto;" />

   <title>GRPO and PPO Notes</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 40px;
      background-color: #f9f9f9;
      color: #333;
    }

    h1, h2 {
      color: #2c3e50;
    }

    code {
      background-color: #eee;
      padding: 2px 5px;
      border-radius: 4px;
    }

    ul {
      margin-left: 20px;
    }
  </style>
</head>
<body>

  <h1>GRPO vs PPO: Key Concepts</h1>

  <h2>1. What is GRPO?</h2>
  <p><strong>GRPO</strong> (Gradient-Regularized Policy Optimization) is designed to:</p>
  <ul>
    <li>Improve computational efficiency through mathematical reasoning.</li>
    <li>Optimize memory usage by eliminating the need for a value model.</li>
    <li>Sample multiple outputs per prompt and average their rewards to guide updates.</li>
  </ul>

  <h3>Steps in GRPO</h3>
  <ol>
    <li>Sample <code>n</code> completions from current policy for each prompt.</li>
    <li>Evaluate each with a reward model ‚Üí get <code>r‚ÇÅ, r‚ÇÇ, ..., r‚Çô</code>.</li>
    <li>Compute average reward: 
      <div class="formula">
        RÃÑ = (1/n) √ó ‚àë r·µ¢
      </div>
    </li>
    <li>Use policy gradient method to maximize expected reward while regularizing against a reference model.</li>
  </ol>

  <h2>2. Differences from PPO</h2>
  <ul>
    <li><strong>PPO</strong> uses a value model for reward estimation.</li>
    <li><strong>GRPO</strong> eliminates the value model and uses averaged sampled rewards instead.</li>
  </ul>

  <h2>3. Techniques Used</h2>
  <ul>
    <li><strong>Regularization</strong>: Prevents over-optimization by penalizing large deviations from a reference policy.</li>
    <li><strong>Instruction Fine-tuning</strong>: Trains models to follow instructions better.</li>
    <li><strong>Rejection Sampling</strong>: Selects high-quality responses by discarding poor ones.</li>
    <li><strong>Policy Gradient Algorithms</strong>: Core reinforcement learning method to update the policy.</li>
  </ul>

  <h2>4. Regularization using KL-Divergence</h2>
  <p>KL regularization ensures the updated policy doesn‚Äôt diverge too much from the reference. The KL divergence is calculated as:</p>

  <p><strong>KL(P || Q)</strong> = ‚àë P(x) log [ P(x) / Q(x) ]</p>

  <p>Where:</p>
  <ul>
    <li><code>P(x)</code> = new policy</li>
    <li><code>Q(x)</code> = reference policy (e.g., initial supervised model)</li>
  </ul>

  <h1>Understanding Instruction Finetuning (IFT)</h1>

  <h2>What is Instruction Finetuning (IFT)?</h2>
  <p>Imagine you have a super smart student who knows a lot of general stuff, but they're not very good at following specific instructions yet. Instruction Finetuning is like giving that student a special training course where they learn to understand and follow instructions perfectly.</p>
  <p>In the world of AI, IFT teaches a large language model (LLM) to be really good at understanding your "instructions" or "prompts" and giving you the kind of response you expect. It uses a specific way of organizing the information we give to the AI, like a script for a play.</p>

  <h2>How Does IFT Work? (Chat Templates and Roles)</h2>
  <p>Think of it like a conversation with different people involved. In IFT, every piece of information we give to the AI is assigned a "role." There are three main roles:</p>

  <ul>
    <li>
      <p><strong class="role role-system">System:</strong> This is like setting the overall mood or rules for the AI. It's the prompt that tells the AI how to behave or what kind of personality it should have. It's often hidden from the user, but it's super important for guiding the AI's behavior.</p>
      <div class="example-box">
        <strong>Example:</strong> <span class="role-system">"You are a helpful and friendly assistant. Always provide concise answers."</span>
      </div>
    </li>
    <li>
      <p><strong class="role role-user">User:</strong> This is YOU! It's any message or question you send to the AI. This is where you give your instructions or ask for information.</p>
      <div class="example-box">
        <strong>Example:</strong> <span class="role-user">"Can you explain photosynthesis to me in simple terms?"</span>
      </div>
    </li>
    <li>
      <p><strong class="role role-assistant">Assistant:</strong> This is the AI's response back to you. It holds what the AI says or generates based on your instructions and the system's guidance.</p>
      <div class="example-box">
        <strong>Example:</strong> <span class="role-assistant">"Sure! Photosynthesis is how plants make their own food using sunlight, water, and carbon dioxide."</span>
      </div>
    </li>
  </ul>

  <p>These roles create a structured "chat template" where the AI learns how different parts of a conversation fit together. It's like teaching a child the difference between who asks a question, who gives an answer, and what the general rules of polite conversation are.</p>

  <h2>Why is IFT Important? (Key Principles)</h2>
  <p>There are a couple of big ideas that make IFT so powerful:</p>

  <ol>
    <li>
      <p><strong>High-Quality Data is King:</strong> Imagine trying to teach someone to cook with bad recipes. They'll never learn! The same goes for AI. The better and more accurate the examples (the "data" in our notes) we give the AI during IFT, the better it will perform. If we show it lots of good instructions and good answers, it learns to give good answers itself.</p>
    </li>
    <li>
      <p><strong>Lots of Practice Makes Perfect (~1 Million Prompts!):</strong> It takes a lot of practice for an AI to become truly amazing. We're talking about feeding it around <strong>1 million (that's 1,000,000!)</strong> different instructions or prompts during the IFT process. This massive amount of training helps the model become super capable, especially for something called <strong>RLHF (Reinforcement Learning from Human Feedback)</strong>, which is another advanced training technique that makes AI even better at being helpful and harmless.</p>
    </li>
  </ol>

  <p>So, in simple terms, Instruction Finetuning is like a specialized training program that teaches AI models to understand and follow instructions by breaking down conversations into specific roles and practicing with a huge amount of high-quality examples. This is a big reason why today's AI can understand your questions and respond so intelligently!</p>

  <p>Hopefully, this makes IFT a little clearer for you! Now you know a bit more about the magic behind those smart AI conversations.</p>

  
</body>
</html>
